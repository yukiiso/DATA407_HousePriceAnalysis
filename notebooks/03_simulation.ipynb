{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Effectiveness of Method A (Ratio Estimation)\n",
    "\n",
    "This simulation demonstrates that when sample sizes are imbalanced across groups (e.g., regions or prefectures), a naive average of all samples can be biased toward groups with more data. In contrast, **Method A (Ratio Estimation)** uses known population-level weights (e.g., housing stock) to correctly estimate the national average.\n",
    "\n",
    "We generate synthetic data with:\n",
    "- 10 groups, each with its own true average house price\n",
    "- Different sample sizes per group to simulate data imbalance\n",
    "- Known housing stock weights per group\n",
    "\n",
    "We then compare:\n",
    "- The **true national average** (based on true group means and weights)\n",
    "- The **naive sample mean** (ignoring group imbalance)\n",
    "- The **weighted estimate** using Method A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True National Average:       3773.78\n",
      "Naive Sample Mean:           4041.12\n",
      "Weighted Estimate (Method A): 3773.99\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(11888757)\n",
    "\n",
    "# Define 10 synthetic groups (e.g., regions or prefectures)\n",
    "groups = [f\"Group_{i}\" for i in range(10)]\n",
    "\n",
    "# Assign a true average price (μ) to each group, between 20M and 60M yen\n",
    "true_means = np.random.uniform(2000, 6000, size=10)\n",
    "\n",
    "# Define the true housing stock weights (population-level weights)\n",
    "true_weights = np.random.randint(1000, 5000, size=10)\n",
    "true_weights = true_weights / true_weights.sum()  # Normalize to sum to 1\n",
    "\n",
    "# Assign sample sizes for each group (to simulate data imbalance)\n",
    "sample_sizes = np.random.randint(100, 10000, size=10)\n",
    "\n",
    "# Generate synthetic transaction data for each group\n",
    "data = []\n",
    "for i in range(10):\n",
    "    mu = true_means[i]\n",
    "    sigma = mu * 0.1  # Add 10% noise\n",
    "    n = sample_sizes[i]\n",
    "    samples = np.random.normal(mu, sigma, size=n)\n",
    "    for value in samples:\n",
    "        data.append({\n",
    "            'group': groups[i],\n",
    "            'value': value\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the sample mean for each group\n",
    "group_means = df.groupby(\"group\")[\"value\"].mean()\n",
    "\n",
    "# Naive sample mean (ignores imbalance in sample sizes)\n",
    "naive_mean = df[\"value\"].mean()\n",
    "\n",
    "# Ratio Estimation (Method A): use group means and population weights\n",
    "weighted_mean = sum(group_means[g] * true_weights[i] for i, g in enumerate(groups))\n",
    "\n",
    "# Ground truth: weighted average using true means and true weights\n",
    "true_national_mean = sum(true_means * true_weights)\n",
    "\n",
    "# Display results\n",
    "print(f\"True National Average:       {true_national_mean:.2f}\")\n",
    "print(f\"Naive Sample Mean:           {naive_mean:.2f}\")\n",
    "print(f\"Weighted Estimate (Method A): {weighted_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Interpretation\n",
    "\n",
    "The results clearly show that the **naive sample mean** overestimates the true national average due to sample imbalance. Groups with larger sample sizes have a disproportionate influence, even if they are not proportionally large in the actual housing stock.\n",
    "\n",
    "In contrast, **Method A (Ratio Estimation)** produces a result very close to the **true national average**, confirming that applying proper weights based on housing stock is essential for an accurate and unbiased estimation.\n",
    "\n",
    "This supports the validity of using ratio estimation when transaction data is unevenly distributed across regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 When Method B Results in a Narrower Confidence Interval\n",
    "\n",
    "This simulation demonstrates a scenario in which **Method B (cluster-based estimation)** achieves a **narrower confidence interval (CI)** than **Method A (group-based estimation)**.\n",
    "\n",
    "To create such a case:\n",
    "- We construct a synthetic population of 12 groups, deliberately divided into three clearly distinct clusters:  \n",
    "  **Low price (~2000), Mid price (~5000), and High price (~10000).**\n",
    "- Each group has a sample drawn from a normal distribution centered around its true mean with moderate noise (10% of mean).\n",
    "- We apply both Method A and Method B to estimate the national average price and calculate their confidence intervals.\n",
    "\n",
    "This setup is meant to reflect a market where regional price differences are substantial and cluster structures are meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True mean: 6081.67\n",
      "\n",
      "=== Method A (Group-based) ===\n",
      "Mean: 6146.97\n",
      "SE:   5.01\n",
      "95% CI: (6137.15, 6156.79)\n",
      "Distance from true mean: 65.30\n",
      "\n",
      "=== Method B (Cluster-based) ===\n",
      "Mean: 6146.97\n",
      "SE:   4.83\n",
      "95% CI: (6137.51, 6156.43)\n",
      "Distance from true mean: 65.30\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(11888757)\n",
    "\n",
    "# Step 1: Define 3 very clear clusters in true_means\n",
    "num_groups = 12\n",
    "group_ids = [f\"G{i}\" for i in range(num_groups)]\n",
    "\n",
    "# Cluster 1: 2000〜2200, Cluster 2: 4000〜4200, Cluster 3: 8000〜8200\n",
    "true_means = np.array([\n",
    "    2000, 2020, 2040, 2060,     # Cluster 0: Low\n",
    "    5000, 5020, 5040, 5060,     # Cluster 1: Mid\n",
    "    10000, 10020, 10040, 10060  # Cluster 2: High\n",
    "])\n",
    "true_weights = np.random.randint(1000, 5000, size=num_groups)\n",
    "true_weights = true_weights / true_weights.sum()\n",
    "\n",
    "# Step 2: Generate sample data with *very* low variance\n",
    "sample_sizes = np.random.randint(1000, 3000, size=num_groups)\n",
    "data = []\n",
    "\n",
    "for i in range(num_groups):\n",
    "    mu = true_means[i]\n",
    "    sigma = mu * 0.1  # noise (10%)\n",
    "    n = sample_sizes[i]\n",
    "    samples = np.random.normal(mu, sigma, size=n)\n",
    "    for val in samples:\n",
    "        data.append({'group': group_ids[i], 'value': val})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Compute group-level statistics\n",
    "group_stats = df.groupby(\"group\").agg(\n",
    "    sample_mean=('value', 'mean'),\n",
    "    sample_var=('value', 'var'),\n",
    "    n=('value', 'count')\n",
    ").reset_index()\n",
    "group_stats['true_mean'] = true_means\n",
    "group_stats['weight'] = true_weights\n",
    "\n",
    "# Step 4: Method A (group-based)\n",
    "def method_a(df):\n",
    "    wm = np.sum(df['sample_mean'] * df['weight'])\n",
    "    se = np.sqrt(np.sum((df['weight']**2 * df['sample_var']) / df['n']))\n",
    "    ci = (wm - 1.96 * se, wm + 1.96 * se)\n",
    "    return wm, se, ci\n",
    "\n",
    "mean_a, se_a, ci_a = method_a(group_stats)\n",
    "\n",
    "# Step 5: Cluster formation (KMeans on sample_mean)\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "group_stats['cluster'] = kmeans.fit_predict(group_stats[['sample_mean']])\n",
    "\n",
    "# Step 6: Compute cluster-level stats with weighted average\n",
    "cluster_stats = []\n",
    "\n",
    "for cluster_id, sub_df in group_stats.groupby(\"cluster\"):\n",
    "    cluster_mean = np.average(sub_df['sample_mean'], weights=sub_df['weight'])  \n",
    "    cluster_weight = sub_df['weight'].sum()\n",
    "    values = df[df['group'].isin(sub_df['group'])]['value']\n",
    "    cluster_var = np.var(values, ddof=1)\n",
    "    cluster_n = len(values)\n",
    "\n",
    "    cluster_stats.append({\n",
    "        'cluster': cluster_id,\n",
    "        'mean': cluster_mean,\n",
    "        'var': cluster_var,\n",
    "        'n': cluster_n,\n",
    "        'weight': cluster_weight\n",
    "    })\n",
    "\n",
    "cluster_df = pd.DataFrame(cluster_stats)\n",
    "\n",
    "# Step 7: Method B (cluster-based)\n",
    "def method_b(df):\n",
    "    wm = np.sum(df['mean'] * df['weight'])\n",
    "    se = np.sqrt(np.sum((df['weight']**2 * df['var']) / df['n']))\n",
    "    ci = (wm - 1.96 * se, wm + 1.96 * se)\n",
    "    return wm, se, ci\n",
    "\n",
    "mean_b, se_b, ci_b = method_b(cluster_df)\n",
    "\n",
    "# Step 8: True national mean\n",
    "true_mean = np.sum(true_means * true_weights)\n",
    "\n",
    "# Step 9: Output\n",
    "print(f\"True mean: {true_mean:.2f}\\n\")\n",
    "\n",
    "print(\"=== Method A (Group-based) ===\")\n",
    "print(f\"Mean: {mean_a:.2f}\")\n",
    "print(f\"SE:   {se_a:.2f}\")\n",
    "print(f\"95% CI: ({ci_a[0]:.2f}, {ci_a[1]:.2f})\")\n",
    "print(f\"Distance from true mean: {abs(mean_a - true_mean):.2f}\\n\")\n",
    "\n",
    "print(\"=== Method B (Cluster-based) ===\")\n",
    "print(f\"Mean: {mean_b:.2f}\")\n",
    "print(f\"SE:   {se_b:.2f}\")\n",
    "print(f\"95% CI: ({ci_b[0]:.2f}, {ci_b[1]:.2f})\")\n",
    "print(f\"Distance from true mean: {abs(mean_b - true_mean):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result Interpretation\n",
    "\n",
    "While both methods slightly overestimated the true mean due to noise, **Method B yielded a marginally narrower confidence interval**.\n",
    "\n",
    "This result supports the theoretical claim that:\n",
    "> \"Clustering similar groups can reduce within-cluster variance, leading to a smaller overall standard error and narrower confidence intervals — provided that the clusters are well-defined and internally consistent.\"\n",
    "\n",
    "Such structure-aware estimation may be useful in real-world housing market analysis, where price segmentation across regions can be clearly observed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
